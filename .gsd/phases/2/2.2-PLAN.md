---
phase: 2
plan: 2
wave: 2
---

# Plan 2.2: RAG Pipeline & Cited Answers

## Objective
Combine the Document Ingestion service (ChromaDB) with the LLM Service to form a complete RAG pipeline. Ensure that generated answers explicitly cite the source document snippets (REQ-03).

## Context
- src/services/vector_store.py
- src/services/llm_service.py
- src/api/qa.py
- .gsd/REQUIREMENTS.md

## Tasks

<task type="auto">
  <name>Enhance Vector Store for Search</name>
  <files>src/services/vector_store.py</files>
  <action>
    - Add a `query_collection(collection, query: str, n_results: int = 3) -> list[dict]` function.
    - This function should return the top match texts and their associated metadata (filename, doc_id).
  </action>
  <verify>python -c "from src.services.vector_store import get_collection, query_collection; print(query_collection(get_collection(), 'test query'))"</verify>
  <done>Function successfully queries ChromaDB and returns a structured list of results.</done>
</task>

<task type="auto">
  <name>Implement Contextual RAG Service</name>
  <files>src/services/rag_service.py</files>
  <action>
    - Create a pipeline that accepts a user query.
    - Calls `query_collection` to get context chunks.
    - Constructs a synthesized prompt for the LLM that strictly enforces returning the answer AND the exact citations (e.g., "[Source: filename.pdf]").
    - Calls `LLMService.generate_response` with this augmented prompt.
    - Return a structured schema containing `answer` and `citations`.
  </action>
  <verify>pytest -v tests/test_rag.py</verify>
  <done>Service builds an augmented prompt, queries the LLM, and formats citations successfully.</done>
</task>

<task type="auto">
  <name>Create Grounded QA Endpoint</name>
  <files>src/api/qa.py</files>
  <action>
    - Add a `POST /ask` endpoint.
    - Accepts a query string.
    - Executes the `rag_service` pipeline.
    - Returns a JSON response with the generated text and a list of citations.
  </action>
  <verify>curl -X POST -H "Content-Type: application/json" -d '{"query": "What is in the document?"}' http://127.0.0.1:8000/ask</verify>
  <done>User can query the system and receive a grounded response that cites a previously uploaded document.</done>
</task>

## Success Criteria
- [ ] Semantic search correctly retrieves relevant chunks from ChromaDB.
- [ ] Answers are successfully generated based on retrieved context.
- [ ] The JSON response includes specific citations pointing back to the filename or chunk.
