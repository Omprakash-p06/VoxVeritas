---
phase: 2
plan: 1
wave: 1
---

# Plan 2.1: LLM Engine Integration

## Objective
Integrate the Sarvam-1 2B language model (or a comparable 2B instruction model if Sarvam-1 GGUF is unavailable) via `llama-cpp-python`. This ensures the model runs strictly under the 4GB VRAM limit (REQ-08). 

## Context
- .gsd/SPEC.md
- .gsd/REQUIREMENTS.md
- src/main.py

## Tasks

<task type="auto">
  <name>Install LLM Dependencies</name>
  <files>requirements.txt</files>
  <action>
    - Add `llama-cpp-python` and `huggingface_hub` to `requirements.txt`.
    - Install them in the virtual environment. Use pre-built wheels or ensure `llama-cpp-python` avoids compilation errors by installing it correctly for a CPU/CUDA environment as available.
  </action>
  <verify>python -c "import llama_cpp, huggingface_hub; print('OK')"</verify>
  <done>Dependencies load successfully in python.</done>
</task>

<task type="auto">
  <name>Implement Local LLM Service</name>
  <files>src/services/llm_service.py</files>
  <action>
    - Create an `LLMService` class.
    - On initialization, conditionally download a GGUF quantized model (e.g. 2B model, Q4_K_M) using `huggingface_hub` into a `.data/models/` directory to cache it.
    - Load the model using `llama_cpp.Llama` with appropriate context size (e.g. 2048) and `n_gpu_layers=-1` to offload to the RTX 3050.
    - Implement a `generate_response(prompt: str) -> str` function.
  </action>
  <verify>pytest -v tests/test_llm.py (assuming you write a brief mock/load test)</verify>
  <done>Service can instantiate a Llama model and generate a basic text completion.</done>
</task>

<task type="auto">
  <name>Create Base Generation Endpoint</name>
  <files>src/api/qa.py, src/main.py</files>
  <action>
    - Create a simple `POST /chat` endpoint that takes a generic text prompt and returns the LLM's response.
    - Wire this router into `src/main.py`.
    - This validates that the model loads in the FastAPI context without memory crashes.
  </action>
  <verify>curl -X POST -H "Content-Type: application/json" -d '{"prompt": "Hello!"}' http://127.0.0.1:8000/chat</verify>
  <done>API returns a 200 JSON response containing the generated text from the LLM.</done>
</task>

## Success Criteria
- [ ] Model successfully downloads and caches locally in GGUF format.
- [ ] VRAM usage stays safely under 4GB during inference.
- [ ] `/chat` endpoint completes successfully.
