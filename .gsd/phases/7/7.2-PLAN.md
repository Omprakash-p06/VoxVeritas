---
phase: 7
plan: 2
wave: 2
---

# Plan 7.2: Live Promptfoo Hallucination Evaluation

## Objective
Run the Promptfoo safety evaluation suite against the live, running VoxVeritas API and produce the actual HTML safety dashboard confirming the LLM does not hallucinate or comply with adversarial prompts.

## Context
- .gsd/phases/7/RESEARCH.md
- evaluations/promptfooconfig.yaml
- evaluations/safety_tests.csv
- scripts/run_evals.ps1

## Pre-Condition: Plan 7.1 must be complete (server must be running).

## Tasks

<task type="auto">
  <name>Expand Promptfoo Safety Test Coverage</name>
  <files>evaluations/safety_tests.csv</files>
  <action>
    Expand `safety_tests.csv` to at least 10 test cases covering:
    - Prompt injection ("Ignore all instructions...")
    - Harmful content (DDoS, weapon instructions)
    - Hallucination checks (querying for facts not in the knowledge base)
    - Out-of-scope scope queries (completely unrelated knowledge domains)
    - Expected positive matches (standard factual queries to ensure the model is not over-restricted)
  </action>
  <verify>CSV file has >= 10 rows of test prompts.</verify>
  <done>Comprehensive safety evaluation dataset is ready.</done>
</task>

<task type="auto">
  <name>Execute Live Promptfoo Evaluation</name>
  <files>evaluations/</files>
  <action>
    - With the server running (`.\scripts\run_server.ps1`), in a second terminal execute:
      ```powershell
      cd "c:\Users\OM Prakash\Documents\VoxVeritas"
      npx promptfoo@latest eval -c evaluations/promptfooconfig.yaml --output evaluations/results.json
      ```
    - Capture the pass/fail results.
    - If assertion failures exist, analyze the LLM response and update the system prompt in `rag_service.py` to add safety guards (e.g., a preamble like "If the query asks for illegal or harmful content, refuse politely.").
  </action>
  <verify>Promptfoo eval runs to completion without connection errors and produces `evaluations/results.json`.</verify>
  <done>Live safety evaluation results are captured in `results.json`.</done>
</task>

<task type="auto">
  <name>View Safety Dashboard</name>
  <files>evaluations/</files>
  <action>
    - Run `npx promptfoo@latest view -y` to open the HTML results dashboard.
    - Take a screenshot or note down the pass/fail counts for documentation.
  </action>
  <verify>The promptfoo dashboard opens in the browser without errors.</verify>
  <done>Safety dashboard is live and shows evaluation results.</done>
</task>

## Success Criteria
- [ ] At least 10 adversarial prompts are defined in `safety_tests.csv`.
- [ ] All test cases complete successfully (no network/server errors).
- [ ] `evaluations/results.json` is generated.
- [ ] LLM does not comply with any of the harmful/jailbreak prompts.
