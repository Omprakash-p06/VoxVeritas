---
phase: 4
plan: 1
wave: 1
---

# Plan 4.1: Screen Reader Service & OCR

## Objective
Implement screen capture and Optical Character Recognition (OCR) using zero-VRAM Windows Native APIs, allowing the application to "read" the user's screen (REQ-02).

## Context
- .gsd/SPEC.md
- .gsd/ROADMAP.md
- .gsd/phases/4/RESEARCH.md

## Tasks

<task type="auto">
  <name>Install Native OCR Dependencies</name>
  <files>requirements.txt</files>
  <action>
    - Add `winsdk` and `Pillow` to `requirements.txt`.
    - Install the packages into the virtual environment using pip.
  </action>
  <verify>python -c "import winsdk.windows.media.ocr; from PIL import ImageGrab; print('Libs OK')"</verify>
  <done>Dependencies for screen grabbing and native OCR installed.</done>
</task>

<task type="auto">
  <name>Implement ScreenReaderService</name>
  <files>src/services/screen_reader.py</files>
  <action>
    - Create a `ScreenReaderService` class.
    - Implement a `capture_and_read_screen() -> str` method.
    - Uses `Pillow.ImageGrab` to grab the current screen.
    - Since `winsdk` OCR requires Windows SoftwareBitmaps, an easy bridge in Python is to save the Pillow image to a temporary `.png` file in `.data/temp_image/`, then load it using `winsdk.windows.storage.StorageFile.get_file_from_path_async` and pass to `OcrEngine`.
    - Alternatively use raw bytes if supported, but temp file is completely robust.
    - The method should extract all text chunks detected by the OcrEngine and join them into a unified context string.
  </action>
  <verify>pytest -v tests/test_screen_reader.py</verify>
  <done>Service successfully captures the screen and returns extracted text.</done>
</task>

<task type="auto">
  <name>Integrate Screen Context into RAG API</name>
  <files>src/api/qa.py, src/api/voice.py</files>
  <action>
    - Enhance `/ask_voice` endpoint. If the user query contains keywords like "screen", "what am I looking at", or "on my screen", or if we just accept an optional boolean parameter `include_screen=True`.
    - Let's add an optional form field `read_screen: bool = False` to `/ask_voice`. 
    - If `read_screen` is True, invoke `ScreenReaderService.capture_and_read_screen()`, and concatenate the extracted screen text directly into the RAG context sent to the `LLMService`.
  </action>
  <verify>Ensure API endpoints accept the new parameter and route screen text to the LLM context.</verify>
  <done>LLM is successfully injected with screen text context when the feature is actively requested.</done>
</task>

## Success Criteria
- [ ] `winsdk` is successfully installed and loads Windows OCR Engine.
- [ ] `ScreenReaderService` reliably detects text on the active monitor without crashing.
- [ ] Voice pipeline can accept the `read_screen` flag to augment its normal RAG data with real-time screen data.
