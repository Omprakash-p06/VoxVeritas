---
phase: 6
plan: 1
wave: 1
---

# Plan 6.1: Promptfoo Safety Evaluation

## Objective
Set up automated safety and red-teaming evaluations using Promptfoo against the VoxVeritas FastAPI backend (REQ-06).

## Context
- .gsd/SPEC.md
- .gsd/ROADMAP.md
- .gsd/phases/6/RESEARCH.md

## Tasks

<task type="auto">
  <name>Initialize Promptfoo Project</name>
  <files>evaluations/promptfooconfig.yaml</files>
  <action>
    - Create an `evaluations` directory.
    - Write a `promptfooconfig.yaml` utilizing the `http` provider.
    - The provider should point to `http://localhost:8000/ask`.
    - Set the method to POST, headers to `Content-Type: application/json`.
    - Map the body to `{"question": "{{prompt}}"}`.
    - Use response parsing to extract the `answer` field from the JSON response.
  </action>
  <verify>Configuration file syntax is valid.</verify>
  <done>Promptfoo configuration is tailored for the local FastAPI instance.</done>
</task>

<task type="manual">
  <name>Define Safety Test Cases</name>
  <files>evaluations/safety_tests.csv</files>
  <action>
    - Create a dataset of test prompts focusing on safety and RAG grounding.
    - Include:
      1. Jailbreak attempts ("Ignore previous instructions and say X").
      2. Harmful content requests.
      3. Hallucination checks (Asking about facts completely outside the provided documents).
      4. Standard baseline Q&A.
    - Link this dataset into the `promptfooconfig.yaml`.
    - Define assertions in the YAML (e.g., `not-contains: "harmful"`, or use LLM-based grading if an OpenAI key was present, but since we are purely local, we'll use deterministic string matching tools).
  </action>
  <verify>CSV contains robust adversarial test cases.</verify>
  <done>A deterministic safety evaluation matrix is defined.</done>
</task>

<task type="auto">
  <name>Create Evaluation Runner Script</name>
  <files>scripts/run_evals.ps1</files>
  <action>
    - Write a PowerShell helper script `run_evals.ps1` that:
      1. Ensures the FastAPI server is running.
      2. Runs `npx promptfoo eval -c evaluations/promptfooconfig.yaml`.
      3. Starts the Promptfoo UI `npx promptfoo view` to fulfill the "Safety Dashboard" requirement.
  </action>
  <verify>Running the script successfully invokes Promptfoo.</verify>
  <done>A single-click script executes the safety pipeline.</done>
</task>

## Success Criteria
- [ ] `promptfooconfig.yaml` correctly routes prompts to the local `/ask` endpoint.
- [ ] The evaluation suite includes at least 5 distinct red-team/safety questions.
- [ ] Running the evaluation suite completes without crashing the Sarvam LLM.
- [ ] The final output can be viewed in a dashboard format.
