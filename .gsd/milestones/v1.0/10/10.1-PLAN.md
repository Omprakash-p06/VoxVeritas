---
phase: 10
plan: 1
wave: 1
---

# Plan 10.1: Engine Fixes & GPU Acceleration

## Objective
Address foundational processing errors including fixing the RAG distance threshold, resolving Llama 3/Sarvam prompt hallucination bugs, and enforcing the installation of a CUDA-enabled llama.cpp package.

## Context
- .gsd/SPEC.md
- src/services/vector_store.py
- src/api/qa.py
- src/services/rag_service.py

## Tasks

<task type="auto">
  <name>Enable GPU Acceleration via pip</name>
  <files>
    requirements.txt
  </files>
  <action>
    - Ensure `llama-cpp-python` is compiled with GPU support by running:
      `pip uninstall -y llama-cpp-python`
      `pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 --force-reinstall`
  </action>
  <verify>python -c "import llama_cpp; print(llama_cpp.llama_supports_gpu_offload())"</verify>
  <done>Returns True indicating the GPU wheel is active.</done>
</task>

<task type="auto">
  <name>Fix ChromaDB RAG Null Distance Bug</name>
  <files>
    src/services/vector_store.py
  </files>
  <action>
    - Locate `max_distance: float = 0.65` in `query_collection()`.
    - Change it to `max_distance: float = 1.6` or remove the check entirely. `all-MiniLM-L6-v2` L2 distances often evaluate around 1.0.
  </action>
  <verify>Run the backend query against ingested docs to ensure `rag_service.py` receives context array.</verify>
  <done>Vector store queries accurately return text chunks.</done>
</task>

<task type="auto">
  <name>Fix Chat Hallucination Prompt Format & API Binding</name>
  <files>
    src/api/qa.py
    src/services/rag_service.py
  </files>
  <action>
    - In `qa.py`, modify `@router.post("/chat")` to inject `mode="chat"` when it dispatches to `LLMService.generate_response()` (bypasses RAG mode fallback).
    - In `rag_service.py`, modify the Llama 3 Chat mode generation string from `<|im_start|>` to `<|start_header_id|>user<|end_header_id|>\n\n{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n`.
  </action>
  <verify>POST /chat with a simple payload responds correctly without generating random languages.</verify>
  <done>/chat correctly accesses Llama-3 metrics without confusing ChatML tokens.</done>
</task>

## Success Criteria
- [ ] Model inference triggers GPU.
- [ ] RAG effectively matches contextual text.
- [ ] Chat responds accurately without Bengali/Gibberish.
