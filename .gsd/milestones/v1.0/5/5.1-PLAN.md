---
phase: 5
plan: 1
wave: 1
---

# Plan 5.1: Desktop Frontend Dashboard

## Objective
Build a modern, aesthetic web interface served directly by the FastAPI backend to interact with the VoxVeritas Voice-to-Voice pipeline.

## Context
- .gsd/SPEC.md
- .gsd/ROADMAP.md (Pivoting Phase 5 to Frontend per user request)
- .gsd/phases/5/RESEARCH.md

## Tasks

<task type="auto">
  <name>Configure Static Asset Serving</name>
  <files>src/main.py</files>
  <action>
    - Import `StaticFiles` from `fastapi.staticfiles`.
    - Mount a `static/` directory at the `/` (root) path in `main.py` so that visiting `http://localhost:8000` serves `index.html`.
    - Ensure CORS is configured to allow local browser interactions if needed.
  </action>
  <verify>curl http://localhost:8000/ yields the HTML file without a 404.</verify>
  <done>FastAPI successfully serves the frontend assets.</done>
</task>

<task type="manual">
  <name>Create Frontend UI Structure</name>
  <files>src/static/index.html, src/static/style.css, src/static/app.js</files>
  <action>
    - **index.html**: Create a modern layout featuring:
      - A prominent "Hold to Speak" or "Click to Record" microphone button.
      - A toggle switch for "Enable Screen Reading (Zero-VRAM Native OCR)".
      - A section to display the Live Transcription (`transcription`).
      - A section to display the LLM's Answer (`answer`) with citations.
      - A hidden `<audio>` element to automatically play the returned Base64 Synthesized speech.
    - **style.css**: Apply a premium "glassmorphism" dark-mode aesthetic (deep purples/blacks, blurred backgrounds, smooth pulsing animations when the microphone is recording) to meet the "WOW" factor.
    - **app.js**: Wire up the logic:
      - Use `navigator.mediaDevices.getUserMedia({ audio: true })`.
      - Record the audio stream using `MediaRecorder`.
      - On stop, package the audio Blob into `FormData`.
      - Append the `read_screen` boolean based on the UI toggle.
      - `POST` to `/ask_voice`.
      - Parse the JSON response, update the UI with transcription/answer, and play the `audio_base64` string as a data URI (`data:audio/wav;base64,...`).
  </action>
  <verify>Open `http://localhost:8000` in a browser, click record, speak, and ensure the UI updates and the audio plays back.</verify>
  <done>Frontend seamlessly captures mic and communicates with the backend Voice API.</done>
</task>

<task type="auto">
  <name>Update Roadmap</name>
  <files>.gsd/ROADMAP.md</files>
  <action>
    - Formally swap Phase 5 (Safety) and Phase 6 (Frontend) in the ROADMAP.md to reflect this pivot.
  </action>
  <verify>ROADMAP reflects the new phase order.</verify>
  <done>Documentation aligns with execution.</done>
</task>

## Success Criteria
- [ ] Navigating to `http://localhost:8000` loads a beautiful, responsive UI.
- [ ] The user can click a button, speak into their microphone, and the voice is recorded.
- [ ] The audio is successfully posted to `/ask_voice`.
- [ ] The frontend dynamically displays the Whisper transcription and Sarvam's answer.
- [ ] The frontend automatically plays the Kokoro synthesized audio response to the user.
