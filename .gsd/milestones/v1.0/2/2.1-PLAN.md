---
phase: 2
plan: 1
wave: 1
---

# Plan 2.1: LLM Engine Integration

## Objective
Integrate the Sarvam-1 2B language model in GGUF format via `llama-cpp-python`. This ensures the model runs strictly under the 4GB VRAM limit (REQ-08). 

## Context
- .gsd/SPEC.md
- .gsd/REQUIREMENTS.md
- src/main.py

## Tasks

<task type="auto">
  <name>Cleanup Placeholder Models</name>
  <files>.data/models/*</files>
  <action>
    - Delete any previously downloaded placeholder models (like `qwen2.5-1.5b-instruct-q4_k_m.gguf`) to free up disk space before downloading the Sarvam model.
  </action>
  <verify>ls -lh .data/models/</verify>
  <done>The models directory is clear of old placeholder `.gguf` files.</done>
</task>

<task type="auto">
  <name>Install LLM Dependencies</name>
  <files>requirements.txt</files>
  <action>
    - Add `llama-cpp-python` and `huggingface_hub` to `requirements.txt`.
    - Install them in the virtual environment. Use pre-built wheels or ensure `llama-cpp-python` avoids compilation errors by installing it correctly for a CPU/CUDA environment as available.
  </action>
  <verify>python -c "import llama_cpp, huggingface_hub; print('OK')"</verify>
  <done>Dependencies load successfully in python.</done>
</task>

<task type="auto">
  <name>Implement Local LLM Service</name>
  <files>src/services/llm_service.py</files>
  <action>
    - Create/Update an `LLMService` class.
    - Set the repo to `bartowski/sarvam-1-GGUF` and the filename to `sarvam-1-Q4_K_M.gguf`.
    - Load the model using `llama_cpp.Llama` with appropriate context size (e.g. 2048) and `n_gpu_layers=-1` to offload to the RTX 3050.
    - Implement a `generate_response(prompt: str) -> str` function.
  </action>
  <verify>pytest -v tests/test_llm.py</verify>
  <done>Service can instantiate a Llama model and generate a basic text completion constraint-free.</done>
</task>

<task type="auto">
  <name>Create Base Generation Endpoint</name>
  <files>src/api/qa.py, src/main.py</files>
  <action>
    - Create a simple `POST /chat` endpoint that takes a generic text prompt and returns the LLM's response.
    - Wire this router into `src/main.py`.
    - This validates that the model loads in the FastAPI context without memory crashes.
  </action>
  <verify>curl -X POST -H "Content-Type: application/json" -d '{"prompt": "Hello!"}' http://127.0.0.1:8000/chat</verify>
  <done>API returns a 200 JSON response containing the generated text from the LLM.</done>
</task>

## Success Criteria
- [ ] Model successfully downloads and caches locally in GGUF format.
- [ ] VRAM usage stays safely under 4GB during inference.
- [ ] `/chat` endpoint completes successfully.
