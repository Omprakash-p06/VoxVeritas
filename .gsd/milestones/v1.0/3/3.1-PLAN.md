---
phase: 3
plan: 1
wave: 1
---

# Plan 3.1: Speech-to-Text (STT) Integration

## Objective
Integrate the OpenAI Whisper model locally to transcribe multilingual spoken audio into text, forming the input layer of the voice-first pipeline (REQ-01, REQ-05).

## Context
- .gsd/SPEC.md
- .gsd/phases/3/RESEARCH.md

## Tasks

<task type="auto">
  <name>Install STT Dependencies</name>
  <files>requirements.txt</files>
  <action>
    - Add `openai-whisper` to `requirements.txt`.
    - Check if ffmpeg is accessible.
    - Install package via pip in the virtual environment.
  </action>
  <verify>python -c "import whisper; print('Whisper imported')"</verify>
  <done>Whisper libraries are installed and accessible in Python.</done>
</task>

<task type="auto">
  <name>Implement STT Service</name>
  <files>src/services/stt_service.py</files>
  <action>
    - Create `STTService` class.
    - Load the `base` or `small` multilingual Whisper model into memory on initialization (offload to CPU or GPU, taking care not to exceed 4GB VRAM alongside LLM).
    - Create a method `transcribe_audio(file_path: str) -> str` that processes the file and returns the text.
  </action>
  <verify>pytest -v tests/test_stt.py</verify>
  <done>Service can successfully transcribe a mock audio file into text.</done>
</task>

<task type="auto">
  <name>Create STT Endpoint</name>
  <files>src/api/voice.py, src/main.py</files>
  <action>
    - Create `POST /transcribe` endpoint that accepts an `UploadFile` (audio), saves it temporarily, and returns the transcribed text.
    - Register router in `main.py`.
  </action>
  <verify>curl -X POST -F "file=@tests/fixtures/sample.wav" http://127.0.0.1:8000/transcribe</verify>
  <done>Endpoint receives audio and returns transcribed JSON payload.</done>
</task>

## Success Criteria
- [ ] Requirements `openai-whisper` are successfully tracked.
- [ ] `STTService` successfully loads model and transcribes audio.
- [ ] `POST /transcribe` endpoint is functional.
