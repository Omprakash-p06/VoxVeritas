---
phase: 3
plan: 3
wave: 3
---

# Plan 3.3: End-to-End Voice RAG Pipeline

## Objective
Stitch together the STT (Whisper), Contextual RAG (Sarvam-1 + ChromaDB), and TTS (Kokoro) services into a single unbroken API pipeline for the live demo (REQ-01).

## Context
- src/api/voice.py
- src/services/stt_service.py
- src/services/rag_service.py
- src/services/tts_service.py

## Tasks

<task type="auto">
  <name>Create Voice-to-Voice API Pipeline</name>
  <files>src/api/voice.py</files>
  <action>
    - Create a `POST /ask_voice` endpoint.
    - It should accept an `UploadFile` (audio recording of a question).
    - Step 1: Pass to `STTService` to get the transcribed text string.
    - Step 2: Pass transcribed text to `RAGService` to get the grounded answer and citations.
    - Step 3: Pass the `answer` string to `TTSService` to generate the audio response.
    - Step 4: Return a tailored JSON or multipart response containing the `transcription`, `answer`, `citations`, and a URL/path to the `.wav` audio file (e.g. `audio_url: /static/output.wav`).
  </action>
  <verify>curl -X POST -F "file=@tests/fixtures/sample_question.wav" http://127.0.0.1:8000/ask_voice</verify>
  <done>A single endpoint successfully orchestrates all three models.</done>
</task>

<task type="auto">
  <name>Pipeline Terminal Verification</name>
  <files>tests/cli_voice.py</files>
  <action>
    - Create an interactive mock script `cli_voice.py`.
    - Provide a local `.wav` file path, have the script hit `/ask_voice`, and automatically play back the resulting audio using a standard python audio player (e.g. `winsound` or `os.system`).
  </action>
  <verify>python tests/cli_voice.py --file path/to/question.wav</verify>
  <done>Script successfully prints the RAG transcriptions and plays the output audio.</done>
</task>

## Success Criteria
- [ ] The `ask_voice` endpoint successfully combines Whisper, ChromaDB, Sarvam, and Kokoro.
- [ ] Real-world test via CLI script confirms the generated audio matches the RAG text.
