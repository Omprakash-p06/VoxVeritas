---
phase: 9
plan: 1
wave: 1
---

# Plan 9.1: Model Downloader & Backend Swapper

## Objective
Provide a unified script to download all necessary models (Sarvam RAG, Llama-3 3B Chat, Whisper, Kokoro) and refactor the `LLMService` to dynamically load/unload models to respect the 4GB VRAM constraint.

## Context
- .gsd/SPEC.md
- .gsd/phases/9/RESEARCH.md
- `src/services/llm_service.py`
- `src/api/health.py`

## Tasks

<task type="auto">
  <name>Create Model Download Script</name>
  <files>scripts/download_models.py</files>
  <action>
    - Write a Python script using `huggingface_hub` to download:
      - `QuantFactory/sarvam-1-GGUF` (`sarvam-1-Q4_K_M.gguf`) -> `.data/models/sarvam-1-Q4_K_M.gguf`
      - `bartowski/Llama-3.2-3B-Instruct-GGUF` (`Llama-3.2-3B-Instruct-Q4_K_M.gguf`) -> `.data/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf`
    - It should also ensure Kokoro (hexgrad/Kokoro-82M) and Whisper models are accounted for or instruct the user.
    - Make sure `huggingface_hub` is in `requirements.txt`.
  </action>
  <verify>python scripts/download_models.py --help</verify>
  <done>Script exists and successfully runs without errors.</done>
</task>

<task type="auto">
  <name>Refactor LLMService for Model Swapping</name>
  <files>src/services/llm_service.py</files>
  <action>
    - Add support for multiple models: "rag" (`sarvam-1-Q4_K_M.gguf`) and "chat" (`Llama-3.2-3B-Instruct-Q4_K_M.gguf`).
    - Add a `load_model(model_type)` function.
    - If a requested `model_type` is different from the currently loaded model:
      - `del self.llm`
      - Call python garbage collector (`gc.collect()`)
      - Load the new model via `Llama(model_path=...)`.
    - Expose `get_current_model_info()` returning the active model string.
  </action>
  <verify>pytest -v tests/test_llm_service.py (if applicable) or manual inspection</verify>
  <done>LLMService unloads old models before loading new ones to free VRAM.</done>
</task>

## Success Criteria
- [ ] `download_models.py` can fetch models directly into `.data/models/`.
- [ ] `LLMService` supports hot-swapping between `rag` and `chat` modes seamlessly.
