---
phase: 1
plan: 2
wave: 2
---

# Plan 1.2: Document Ingestion Service

## Objective
Implement background indexing logic, a document parser, and the API endpoint to handle document uploads (PDF, DOCX, TXT), split them into chunks, and store their embeddings in ChromaDB.

## Context
- .gsd/SPEC.md
- src/main.py

## Tasks

<task type="auto">
  <name>Implement Document Parsing and Chunking</name>
  <files>src/services/document_parser.py</files>
  <action>
    - Create a parser for reading text from `.txt`, `.pdf` (using pdfplumber), and `.docx` (using python-docx) files.
    - Create a chunker function that splits extracted text into overlapping chunks (e.g., 500 characters or ~100 words with 50 character overlap).
    - Ensure all code conforms to the Google Python Style Guide, using standard types.
  </action>
  <verify>pytest -v</verify>
  <done>Python module can extract strings from a given test file and successfully chunk the strings.</done>
</task>

<task type="auto">
  <name>Implement Vector Store Initialization</name>
  <files>src/services/vector_store.py</files>
  <action>
    - Initialize a persistent ChromaDB client inside a `.data/chromadb` local directory.
    - Create a collection specifically for VoxVeritas documents.
    - Write a function `add_chunks(collection, chunks: list, metadata: dict)` that uses `sentence-transformers` (specifically an Indic-capable model like `l3cube-pune/indic-sentence-bert-nli` or fallback to `all-MiniLM-L6-v2` for speed if Indic one fails to load) to compute embeddings and add them to ChromaDB.
  </action>
  <verify>python -c "from src.services.vector_store import get_collection; coll = get_collection(); print(coll.count())"</verify>
  <done>ChromaDB successfully initializes a persistent database and collection locally.</done>
</task>

<task type="auto">
  <name>Create Document Upload Endpoint</name>
  <files>src/api/document.py, src/main.py</files>
  <action>
    - Implement `POST /upload` endpoint using `FastAPI`'s `UploadFile`.
    - The endpoint should securely save the file to a `.data/uploads` directory.
    - Then, synchronously pass the file path to `document_parser` and `vector_store` to chunk and embed.
    - Return a JSON response with a unique `doc_id` and an ingestion summary (chunk count).
    - Wire `document.py` router into `main.py`.
  </action>
  <verify>curl -X POST -F "file=@README.md" http://127.0.0.1:8000/upload</verify>
  <done>User can successfully POST a TXT/PDF file and receive a 200 response with chunk count and doc_id.</done>
</task>

## Success Criteria
- [ ] Text can be reliably extracted from PDFs and Word docs.
- [ ] ChromaDB directory is created and persists state between restarts.
- [ ] `POST /upload` stores the file locally and returns an accurate ingestion summary.
